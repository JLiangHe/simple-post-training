+ nproc_per_node=1
+ shift 2
+ torchrun --standalone --nnodes=2
usage: torchrun [-h] [--nnodes NNODES] [--nproc-per-node NPROC_PER_NODE]
                [--rdzv-backend RDZV_BACKEND] [--rdzv-endpoint RDZV_ENDPOINT]
                [--rdzv-id RDZV_ID] [--rdzv-conf RDZV_CONF] [--standalone]
                [--max-restarts MAX_RESTARTS]
                [--monitor-interval MONITOR_INTERVAL]
                [--start-method {spawn,fork,forkserver}] [--role ROLE] [-m]
                [--no-python] [--run-path] [--log-dir LOG_DIR] [-r REDIRECTS]
                [-t TEE] [--local-ranks-filter LOCAL_RANKS_FILTER]
                [--node-rank NODE_RANK] [--master-addr MASTER_ADDR]
                [--master-port MASTER_PORT] [--local-addr LOCAL_ADDR]
                [--logs-specs LOGS_SPECS]
                training_script ...
torchrun: error: the following arguments are required: training_script, training_script_args
+ --nproc_per_node=1 -m verl.trainer.fsdp_sft_trainer data.train_files=/gpfs/radev/home/jh3439/project/simple-post-training/lm_sft/data/processed_data/train.parquet data.val_files=/gpfs/radev/home/jh3439/project/simple-post-training/lm_sft/data/processed_data/test.parquet data.multiturn.enable=true data.multiturn.messages_key=messages data.micro_batch_size=1 data.train_batch_size=128 data.max_length=2048 data.truncation=right model.partial_pretrain=/gpfs/radev/home/jh3439/project/simple-post-training/lm_sft/data/processed_models/meta-llama_Llama-3.1-8B model.enable_gradient_checkpointing=true model.fsdp_config.cpu_offload=false model.fsdp_config.offload_params=false model.fsdp_config.model_dtype=bfloat16 trainer.default_local_dir=/gpfs/radev/home/jh3439/project/simple-post-training/lm_sft/results/meta-llama_Llama-3.1-8B trainer.project_name=verl-post-training-pipeline-sft trainer.experiment_name=sft-llama-3.1-8b-multiturn trainer.default_hdfs_dir=null ulysses_sequence_parallel_size=1 use_remove_padding=true
/var/spool/slurmd/job413412/slurm_script: line 24: --nproc_per_node=1: command not found
