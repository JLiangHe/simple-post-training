Total training steps: 116
Epoch 1/4:   0%|                                                                                                                            | 0/29 [00:00<?, ?it/s]/gpfs/radev/home/jh3439/.conda/envs/llm_base/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
step:1 - train/loss:1.2547674179077148 - train/lr(1e-3):0.0009090909090909092
Epoch 1/4: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [06:38<00:00, 13.76s/it]
step:2 - train/loss:1.4314697980880737 - train/lr(1e-3):0.0018181818181818184
step:3 - train/loss:1.04738450050354 - train/lr(1e-3):0.002727272727272727
step:4 - train/loss:1.3636776208877563 - train/lr(1e-3):0.003636363636363637
step:5 - train/loss:1.182496190071106 - train/lr(1e-3):0.004545454545454545
step:6 - train/loss:1.146875262260437 - train/lr(1e-3):0.005454545454545454
step:7 - train/loss:1.258436679840088 - train/lr(1e-3):0.006363636363636364
step:8 - train/loss:1.4696229696273804 - train/lr(1e-3):0.007272727272727274
step:9 - train/loss:0.9209532141685486 - train/lr(1e-3):0.008181818181818182
step:10 - train/loss:1.2344717979431152 - train/lr(1e-3):0.00909090909090909
step:11 - train/loss:0.9961094856262207 - train/lr(1e-3):0.01
step:12 - train/loss:0.9741504192352295 - train/lr(1e-3):0.009997762161417517
step:13 - train/loss:0.9729926586151123 - train/lr(1e-3):0.009991050648838675
step:14 - train/loss:0.7648680806159973 - train/lr(1e-3):0.009979871469976196
step:15 - train/loss:1.290006399154663 - train/lr(1e-3):0.009964234631709187
step:16 - train/loss:0.9600879549980164 - train/lr(1e-3):0.009944154131125644
step:17 - train/loss:0.8589820861816406 - train/lr(1e-3):0.009919647942993149
step:18 - train/loss:0.9365609884262085 - train/lr(1e-3):0.009890738003669028
step:19 - train/loss:0.8821780681610107 - train/lr(1e-3):0.009857450191464338
step:20 - train/loss:0.9965925216674805 - train/lr(1e-3):0.009819814303479267
step:21 - train/loss:1.0265802145004272 - train/lr(1e-3):0.009777864028930706
step:22 - train/loss:0.7375434637069702 - train/lr(1e-3):0.009731636918995822
step:23 - train/loss:0.8712815642356873 - train/lr(1e-3):0.009681174353198686
step:24 - train/loss:0.7569155693054199 - train/lr(1e-3):0.009626521502369983
step:25 - train/loss:0.9814745783805847 - train/lr(1e-3):0.009567727288213005
step:26 - train/loss:1.022887110710144 - train/lr(1e-3):0.009504844339512096
step:27 - train/loss:1.4484691619873047 - train/lr(1e-3):0.009437928945022772
step:28 - train/loss:0.9783110022544861 - train/lr(1e-3):0.009367041003085651
step:29 - train/loss:0.9613382816314697 - train/lr(1e-3):0.009292243968009331
Epoch 2/4: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [06:36<00:00, 13.69s/it]
step:30 - train/loss:0.8333466053009033 - train/lr(1e-3):0.009213604793270196
step:31 - train/loss:0.9116777777671814 - train/lr(1e-3):0.009131193871579974
step:32 - train/loss:0.6932709217071533 - train/lr(1e-3):0.009045084971874739
step:33 - train/loss:0.9993829727172852 - train/lr(1e-3):0.008955355173281708
step:34 - train/loss:0.8838003873825073 - train/lr(1e-3):0.008862084796122998
step:35 - train/loss:0.9892264604568481 - train/lr(1e-3):0.008765357330018056
step:36 - train/loss:0.78035968542099 - train/lr(1e-3):0.008665259359149132
step:37 - train/loss:0.7535001635551453 - train/lr(1e-3):0.008561880484756725
step:38 - train/loss:1.1667593717575073 - train/lr(1e-3):0.008455313244934324
step:39 - train/loss:0.7609072923660278 - train/lr(1e-3):0.008345653031794291
step:40 - train/loss:0.8413388729095459 - train/lr(1e-3):0.008232998006078998
step:41 - train/loss:0.7637404799461365 - train/lr(1e-3):0.008117449009293667
step:42 - train/loss:0.5533661842346191 - train/lr(1e-3):0.00799910947343957
step:43 - train/loss:0.9102132320404053 - train/lr(1e-3):0.00787808532842837
step:44 - train/loss:0.8365660309791565 - train/lr(1e-3):0.007754484907260513
step:45 - train/loss:0.6245266795158386 - train/lr(1e-3):0.007628418849052523
step:46 - train/loss:0.6946187615394592 - train/lr(1e-3):0.0075000000000000015
step:47 - train/loss:0.5045656561851501 - train/lr(1e-3):0.007369343312364994
step:48 - train/loss:0.913392186164856 - train/lr(1e-3):0.007236565741578163
step:49 - train/loss:0.6372193694114685 - train/lr(1e-3):0.007101786141547829
step:50 - train/loss:0.8234379887580872 - train/lr(1e-3):0.006965125158269619
step:51 - train/loss:0.7154582738876343 - train/lr(1e-3):0.0068267051218319766
step:52 - train/loss:0.6923099756240845 - train/lr(1e-3):0.0066866499369141515
step:53 - train/loss:0.7095478177070618 - train/lr(1e-3):0.006545084971874737
step:54 - train/loss:0.7661665678024292 - train/lr(1e-3):0.006402136946530014
step:55 - train/loss:0.8806471228599548 - train/lr(1e-3):0.006257933818722544
step:56 - train/loss:0.6651681065559387 - train/lr(1e-3):0.006112604669781572
step:57 - train/loss:0.8253757357597351 - train/lr(1e-3):0.005966279588977766
step:58 - train/loss:0.8040809035301208 - train/lr(1e-3):0.005819089557075689
Epoch 3/4: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [06:33<00:00, 13.57s/it]
step:59 - train/loss:0.7925496101379395 - train/lr(1e-3):0.0056711663290882775
step:60 - train/loss:0.6995159983634949 - train/lr(1e-3):0.0055226423163382676
step:61 - train/loss:0.5832074880599976 - train/lr(1e-3):0.0053736504679321225
step:62 - train/loss:0.6787898540496826 - train/lr(1e-3):0.005224324151752576
step:63 - train/loss:0.7031431794166565 - train/lr(1e-3):0.005074797035076319
step:64 - train/loss:0.7241420149803162 - train/lr(1e-3):0.0049252029649236835
step:65 - train/loss:0.753341019153595 - train/lr(1e-3):0.004775675848247427
step:66 - train/loss:0.5310685038566589 - train/lr(1e-3):0.004626349532067879
step:67 - train/loss:0.6392943859100342 - train/lr(1e-3):0.0044773576836617335
step:68 - train/loss:0.5456821322441101 - train/lr(1e-3):0.004328833670911724
step:69 - train/loss:0.6212885975837708 - train/lr(1e-3):0.004180910442924311
step:70 - train/loss:0.5792993903160095 - train/lr(1e-3):0.004033720411022235
step:71 - train/loss:0.5893111824989319 - train/lr(1e-3):0.0038873953302184286
step:72 - train/loss:0.5918209552764893 - train/lr(1e-3):0.0037420661812774575
step:73 - train/loss:0.5712726712226868 - train/lr(1e-3):0.0035978630534699873
step:74 - train/loss:0.547850489616394 - train/lr(1e-3):0.0034549150281252636
step:75 - train/loss:0.7972769141197205 - train/lr(1e-3):0.003313350063085851
step:76 - train/loss:0.5453602075576782 - train/lr(1e-3):0.003173294878168025
step:77 - train/loss:0.6165708303451538 - train/lr(1e-3):0.0030348748417303828
step:78 - train/loss:0.8882986307144165 - train/lr(1e-3):0.0028982138584521732
step:79 - train/loss:0.5956500768661499 - train/lr(1e-3):0.0027634342584218364
step:80 - train/loss:0.6447069644927979 - train/lr(1e-3):0.0026306566876350074
step:81 - train/loss:0.7037757635116577 - train/lr(1e-3):0.0025000000000000014
step:82 - train/loss:0.7258267402648926 - train/lr(1e-3):0.002371581150947476
step:83 - train/loss:0.48982688784599304 - train/lr(1e-3):0.002245515092739488
step:84 - train/loss:0.5256215929985046 - train/lr(1e-3):0.0021219146715716333
step:85 - train/loss:0.6948955059051514 - train/lr(1e-3):0.0020008905265604315
step:86 - train/loss:0.8590429425239563 - train/lr(1e-3):0.0018825509907063327
step:87 - train/loss:0.6909440755844116 - train/lr(1e-3):0.0017670019939210026
Epoch 4/4:  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████    | 28/29 [07:24<00:15, 15.86s/it]
step:88 - train/loss:0.7364389896392822 - train/lr(1e-3):0.0016543469682057106
step:89 - train/loss:0.6265041828155518 - train/lr(1e-3):0.001544686755065677
step:90 - train/loss:0.7932867407798767 - train/lr(1e-3):0.001438119515243277
step:91 - train/loss:0.4524588882923126 - train/lr(1e-3):0.0013347406408508696
step:92 - train/loss:0.6414380669593811 - train/lr(1e-3):0.001234642669981946
step:93 - train/loss:0.5539059042930603 - train/lr(1e-3):0.001137915203877003
step:94 - train/loss:0.6342393755912781 - train/lr(1e-3):0.0010446448267182951
step:95 - train/loss:0.552190899848938 - train/lr(1e-3):0.0009549150281252633
step:96 - train/loss:0.6805336475372314 - train/lr(1e-3):0.0008688061284200266
step:97 - train/loss:0.6680688858032227 - train/lr(1e-3):0.0007863952067298042
step:98 - train/loss:0.7616161704063416 - train/lr(1e-3):0.0007077560319906696
step:99 - train/loss:0.6807870268821716 - train/lr(1e-3):0.0006329589969143517
step:100 - train/loss:0.5511847138404846 - train/lr(1e-3):0.0005620710549772295
step:101 - train/loss:0.6013723611831665 - train/lr(1e-3):0.0004951556604879049
step:102 - train/loss:0.5210244655609131 - train/lr(1e-3):0.0004322727117869951
step:103 - train/loss:0.8704172372817993 - train/lr(1e-3):0.0003734784976300165
step:104 - train/loss:0.6673457026481628 - train/lr(1e-3):0.000318825646801314
step:105 - train/loss:0.4388771653175354 - train/lr(1e-3):0.00026836308100417877
step:106 - train/loss:0.6393665671348572 - train/lr(1e-3):0.0002221359710692961
step:107 - train/loss:0.7322240471839905 - train/lr(1e-3):0.0001801856965207338
step:108 - train/loss:0.6060474514961243 - train/lr(1e-3):0.00014254980853566247
step:109 - train/loss:0.8300419449806213 - train/lr(1e-3):0.00010926199633097157
step:110 - train/loss:0.664726734161377 - train/lr(1e-3):8.035205700685168e-05
step:111 - train/loss:0.5443779230117798 - train/lr(1e-3):5.584586887435739e-05
step:112 - train/loss:0.548772394657135 - train/lr(1e-3):3.576536829081323e-05
step:113 - train/loss:0.6867959499359131 - train/lr(1e-3):2.012853002380466e-05
step:114 - train/loss:0.7313305139541626 - train/lr(1e-3):8.949351161324227e-06
step:115 - train/loss:0.5668067932128906 - train/lr(1e-3):2.237838582483387e-06
step:116 - train/loss:0.5824703574180603 - train/lr(1e-3):0.0
step:116 - val/loss:1.0211857557296753
Final validation metrics: {'val/loss': 1.0211857557296753}
